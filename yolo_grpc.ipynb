{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73046ff",
   "metadata": {},
   "source": [
    "# GRPC Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e7e73-24cb-4f03-9491-a6edcc24f0cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9df000-a171-4652-8160-272f81e49612",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grpcio grpcio-tools opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018339c-f1cc-4f2e-8c41-2b78a6667f42",
   "metadata": {},
   "source": [
    "### Inspecting the gRPC Endpoint\n",
    "\n",
    "Let's check out the gRPC endpoint's model metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17b252-7827-4cae-adb0-f98c9d80bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpc_host = 'modelmesh-serving'\n",
    "grpc_port = 8033\n",
    "model_name = 'yolo'\n",
    "\n",
    "# Confidence threshold, between 0 and 1 (detections with less score won't be retained)\n",
    "conf = 0.2\n",
    "\n",
    "# Intersection over Union Threshold, between 0 and 1 (cleanup overlapping boxes)\n",
    "iou = 0.6\n",
    "\n",
    "image_path = 'images/zidane.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545aa5f4-356f-4e70-b7e6-cd352a68927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import grpc_predict_v2_pb2\n",
    "import grpc_predict_v2_pb2_grpc\n",
    "\n",
    "\n",
    "options = [('grpc.max_receive_message_length', 100 * 1024 * 1024)]\n",
    "channel = grpc.insecure_channel(f\"{grpc_host}:{grpc_port}\", options=options)\n",
    "stub = grpc_predict_v2_pb2_grpc.GRPCInferenceServiceStub(channel)\n",
    "\n",
    "request = grpc_predict_v2_pb2.ModelMetadataRequest(name=model_name)\n",
    "response = stub.ModelMetadata(request)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f645a20-61cc-4180-a642-3e875f04b9a2",
   "metadata": {},
   "source": [
    "### Image Preprocessing Functions\n",
    "\n",
    "First, we need to preprocess and format the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3eacd4-5dda-4311-9e9d-35a1bc6e137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def letterbox(im, color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    new_shape= 640\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, r, (dw, dh)\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    image = image.transpose((2, 0, 1))  # HWC->CHW for PyTorch model\n",
    "    image = np.expand_dims(image, 0)  # Model expects an array of images\n",
    "    image = np.ascontiguousarray(image)  # Speed up things by rewriting the array contiguously in memory\n",
    "    im = image.astype(np.float32)  # Model expects float32 data type\n",
    "    im /= 255  # Convert RGB values [0-255] to [0-1]\n",
    "    return im\n",
    "\n",
    "\n",
    "def getImage(path, size):\n",
    "    return cv2.imread(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd11c7-9986-4564-a87d-5851cf9f8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "image_or = getImage(image_path, 640)\n",
    "letterboxed_image, ratio, dwdh = letterbox(image_or, auto=False)\n",
    "img_data = preprocess(letterboxed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5affbf-36c3-4e17-9788-5fc0904de143",
   "metadata": {},
   "source": [
    "### Results filtering and transformation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ce7be-76f8-49d5-a5c4-409ea9c6b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import classes\n",
    "\n",
    "\n",
    "def xywh2xyxy(xywh):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    xyxy = np.copy(xywh)\n",
    "    xyxy[..., 0] = xywh[..., 0] - xywh[..., 2] / 2  # top left x\n",
    "    xyxy[..., 1] = xywh[..., 1] - xywh[..., 3] / 2  # top left y\n",
    "    xyxy[..., 2] = xywh[..., 0] + xywh[..., 2] / 2  # bottom right x\n",
    "    xyxy[..., 3] = xywh[..., 1] + xywh[..., 3] / 2  # bottom right y\n",
    "    return xyxy\n",
    "\n",
    "def get_overlapping_box(new_box, existing_boxes, iou_threshhold):\n",
    "    overlapping_box_index = -1\n",
    "    for i, existing_box in enumerate(existing_boxes):\n",
    "        overlap_x_min = np.maximum(new_box['box']['xMin'], existing_box['box']['xMin'])\n",
    "        overlap_y_min = np.maximum(new_box['box']['yMin'], existing_box['box']['yMin'])\n",
    "        overlap_x_max = np.minimum(new_box['box']['xMax'], existing_box['box']['xMax'])\n",
    "        overlap_y_max = np.minimum(new_box['box']['yMax'], existing_box['box']['yMax'])\n",
    "\n",
    "        # Find out the width and the height of the intersection box\n",
    "        w = np.maximum(0, overlap_x_max - overlap_x_min + 1)\n",
    "        h = np.maximum(0, overlap_y_max - overlap_y_min + 1)\n",
    "        overlap_area = (w * h)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        max_area = np.maximum(new_box['box']['area'], existing_box['box']['area'])\n",
    "        overlap = overlap_area / max_area\n",
    "\n",
    "        # if the actual boungding box has an overlap bigger than overlapThresh\n",
    "        if overlap > iou_threshhold:\n",
    "            overlapping_box_index = i\n",
    "            break\n",
    "    return overlapping_box_index\n",
    "\n",
    "\n",
    "def detection_arr2dict(item):\n",
    "#   item: [middle_X, middle_Y, width, height, obj_confidence, ...class confidences]\n",
    "    class_index = np.argmax(item[5:])\n",
    "    xyxy = xywh2xyxy(item[:4])\n",
    "    return {\n",
    "        'box': {\n",
    "            'xMin': xyxy[0],\n",
    "            'yMin': xyxy[1],\n",
    "            'xMax': xyxy[2],\n",
    "            'yMax': xyxy[3],\n",
    "            'area': item[2] * item[3]\n",
    "        },\n",
    "        'classIndex': class_index,\n",
    "        'class': classes.coco_classes[class_index],\n",
    "        'label': classes.coco_classes[class_index],\n",
    "        'overallConfidence': item[4],\n",
    "        'classConfidence': item[class_index + 5],\n",
    "        'score': item[4] * item[class_index + 5],\n",
    "    }\n",
    "\n",
    "\n",
    "def map_results(detections, overlap_threshhold=0.6, conf_threshhold=0.2, max_detections=20):\n",
    "    results = []\n",
    "    # num_detections = min(len(detections), max_detections)\n",
    "\n",
    "    for i in range(0, len(detections)):        \n",
    "        if len(results) >= max_detections:\n",
    "            break\n",
    "\n",
    "        d = detection_arr2dict(detections[i])\n",
    "        if d['overallConfidence'] < conf_threshhold:\n",
    "            break\n",
    "            \n",
    "        if d['score'] < conf_threshhold:\n",
    "            continue\n",
    "            \n",
    "        overlapping_box_index = get_overlapping_box(d, results, overlap_threshhold)\n",
    "        # only append if there's no overlapping box\n",
    "        if overlapping_box_index < 0:\n",
    "            results.append(d)\n",
    "        # replace if the new box has higher confidence\n",
    "        elif d['score'] > results[overlapping_box_index]['score']:\n",
    "            results[overlapping_box_index] = d\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80471c-1509-436b-9049-213f6aa52a38",
   "metadata": {},
   "source": [
    "### Request Function\n",
    "\n",
    "Builds and submits our gRPC request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1d001-ff99-414a-95d4-5729d5849298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import classes\n",
    "\n",
    "\n",
    "def transform_filter_results(result_arr):\n",
    "    prediction_columns_number = 5 + len(classes.coco_classes)  # Model returns model returns [xywh, conf, class0, class1, ...]\n",
    "    reshaped_result_arr = result_arr.reshape(1, int(int(result_arr.shape[0])/prediction_columns_number), prediction_columns_number)\n",
    "    sorted_result_arr = (reshaped_result_arr[0][reshaped_result_arr[0][:, 4].argsort()])[::-1]\n",
    "    return map_results(sorted_result_arr)\n",
    "\n",
    "\n",
    "def rhods_grpc_request(img_data):\n",
    "    # request content building\n",
    "    inputs = []\n",
    "    inputs.append(grpc_predict_v2_pb2.ModelInferRequest().InferInputTensor())\n",
    "    inputs[0].name = \"images\"\n",
    "    inputs[0].datatype = \"FP32\"\n",
    "    inputs[0].shape.extend([1, 3, 640, 640])\n",
    "    arr = img_data.flatten()\n",
    "    inputs[0].contents.fp32_contents.extend(arr)\n",
    "\n",
    "    # request building\n",
    "    request = grpc_predict_v2_pb2.ModelInferRequest()\n",
    "    request.model_name = model_name\n",
    "    request.inputs.extend(inputs)\n",
    "\n",
    "    t1 = time.time()\n",
    "    response = stub.ModelInfer(request)\n",
    "    t2 = time.time()\n",
    "    inference_time = t2-t1\n",
    "    \n",
    "    result_arr = np.frombuffer(response.raw_output_contents[0], dtype=np.float32)\n",
    "    return transform_filter_results(result_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d15ffd-a0fb-46c6-a2e2-412acc2bf79c",
   "metadata": {},
   "source": [
    "### Run the Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb769b9-5453-45ad-9102-ea21cf98f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence threshold, between 0 and 1 (detections with less score won't be retained)\n",
    "conf = 0.2\n",
    "# Intersection over Union Threshold, between 0 and 1 (cleanup overlapping boxes)\n",
    "iou = 0.6\n",
    "\n",
    "image_path = 'images/zidane.jpg'\n",
    "\n",
    "image_or = getImage(image_path, 640)\n",
    "letterboxed_image, ratio, dwdh = letterbox(image_or, auto=False)\n",
    "img_data = preprocess(letterboxed_image)\n",
    "results = rhods_grpc_request(img_data)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4cc92-5e32-492d-b26c-0689e224fa13",
   "metadata": {},
   "source": [
    "### Show the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfede9e-b27c-48b0-8546-a07577a4a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import classes\n",
    "import random\n",
    "\n",
    "def draw_result(img, ratio, dwdh, detections):\n",
    "    names = classes.coco_classes\n",
    "    colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}   \n",
    "    for i,(d) in enumerate(detections):\n",
    "        box = np.array([d['box']['xMin'], d['box']['yMin'], d['box']['xMax'], d['box']['yMax']])\n",
    "        box -= np.array(dwdh*2)\n",
    "        box /= ratio\n",
    "        box = box.round().astype(np.int32).tolist()\n",
    "        cls_id = int(d['classIndex'])\n",
    "        score = round(float(d['score']),3)\n",
    "        name = names[cls_id]\n",
    "        color = colors[name]\n",
    "        name += ' '+str(score)\n",
    "        cv2.rectangle(img,box[:2],box[2:],color,2)\n",
    "        cv2.putText(img,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[0, 255, 0],thickness=2) \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd41e9-4a77-4d8f-8eca-75f2bdf52114",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = draw_result(image_or, ratio, dwdh, results)  # Draw the boxes from results\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(24, 12)\n",
    "plt.axis('off')\n",
    "plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc0634-3e29-4d01-91f5-2b4912705b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

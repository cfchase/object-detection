{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ingest Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os import environ, listdir, path, unlink\n",
    "from shutil import rmtree\n",
    "import yaml\n",
    "\n",
    "from openimages.download import download_dataset\n",
    "\n",
    "\n",
    "def ingest_data(data_folder='./data', limit=0):\n",
    "    _clean_folder(data_folder)\n",
    "    class_labels = _read_class_labels('configuration.yaml')\n",
    "\n",
    "    print('Commencing data ingestion.')\n",
    "\n",
    "    limit = limit or int(environ.get('sample_count', 100))\n",
    "    download_folder = f'{data_folder}/download'\n",
    "\n",
    "    download_dataset(\n",
    "        download_folder,\n",
    "        class_labels=class_labels,\n",
    "        annotation_format='darknet',\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "    print('data ingestion done')\n",
    "\n",
    "\n",
    "def _clean_folder(folder):\n",
    "    print(f'Cleaning folder {folder}')\n",
    "\n",
    "    for filename in listdir(folder):\n",
    "        file_path = path.join(folder, filename)\n",
    "        try:\n",
    "            if path.isfile(file_path) or path.islink(file_path):\n",
    "                unlink(file_path)\n",
    "            elif path.isdir(file_path):\n",
    "                rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "\n",
    "def _read_class_labels(configuration_file_path):\n",
    "    with open(configuration_file_path, 'r') as config_file:\n",
    "        config = yaml.load(config_file.read(), Loader=yaml.SafeLoader)\n",
    "\n",
    "    class_labels = config['names']\n",
    "    return class_labels\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ingest_data(data_folder='./data')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Process Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from math import floor\n",
    "from os import makedirs, path\n",
    "from shutil import copy\n",
    "import yaml\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "def preprocess_data(data_folder='./data'):\n",
    "    print('preprocessing data')\n",
    "\n",
    "    for folder in ['images', 'labels']:\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            local_folder = f'{data_folder}/{folder}/{split}'\n",
    "            if not path.exists(local_folder):\n",
    "                makedirs(local_folder)\n",
    "\n",
    "    download_folder = f'{data_folder}/download'\n",
    "    class_labels = _read_class_labels('configuration.yaml')\n",
    "\n",
    "    folder_names = [class_name.lower() for class_name in class_labels]\n",
    "    images = [\n",
    "        _get_filenames(f'{download_folder}/{folder_name}/images')\n",
    "        for folder_name in folder_names\n",
    "    ]\n",
    "\n",
    "    duplicates_0_1 = images[0] & images[1]\n",
    "    duplicates_1_2 = images[1] & images[2]\n",
    "    duplicates_2_0 = images[2] & images[0]\n",
    "\n",
    "    images[0] -= duplicates_0_1\n",
    "    images[1] -= duplicates_1_2\n",
    "    images[2] -= duplicates_2_0\n",
    "\n",
    "    random.seed(42)\n",
    "    train_ratio = 0.75\n",
    "    val_ratio = 0.125\n",
    "    for i, image_set in enumerate(images):\n",
    "        image_list = list(image_set)\n",
    "        random.shuffle(image_list)\n",
    "        train_size = floor(train_ratio * len(image_list))\n",
    "        val_size = floor(val_ratio * len(image_list))\n",
    "        _split_dataset(\n",
    "            download_folder,\n",
    "            data_folder,\n",
    "            folder_names[i],\n",
    "            image_list,\n",
    "            train_size=train_size,\n",
    "            val_size=val_size,\n",
    "        )\n",
    "\n",
    "    print('data processing done')\n",
    "\n",
    "\n",
    "def _read_class_labels(configuration_file_path):\n",
    "    with open(configuration_file_path, 'r') as config_file:\n",
    "        config = yaml.load(config_file.read(), Loader=yaml.SafeLoader)\n",
    "\n",
    "    class_labels = config['names']\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "def _get_filenames(folder):\n",
    "    filenames = set()\n",
    "\n",
    "    for local_path in glob(path.join(folder, '*.jpg')):\n",
    "        filename = path.split(local_path)[-1]\n",
    "        filenames.add(filename)\n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "def _split_dataset(\n",
    "        download_folder, data_folder, item, image_names, train_size, val_size):\n",
    "\n",
    "    for i, image_name in enumerate(image_names):\n",
    "        # Label filename\n",
    "        label_name = image_name.replace('.jpg', '.txt')\n",
    "\n",
    "        # Split into train, val, or test\n",
    "        if i < train_size:\n",
    "            split = 'train'\n",
    "        elif i < train_size + val_size:\n",
    "            split = 'val'\n",
    "        else:\n",
    "            split = 'test'\n",
    "\n",
    "        # Source paths\n",
    "        source_image_path = f'{download_folder}/{item}/images/{image_name}'\n",
    "        source_label_path = f'{download_folder}/{item}/darknet/{label_name}'\n",
    "\n",
    "        # Destination paths\n",
    "        target_image_folder = f'{data_folder}/images/{split}'\n",
    "        target_label_folder = f'{data_folder}/labels/{split}'\n",
    "\n",
    "        # Copy files\n",
    "        copy(source_image_path, target_image_folder)\n",
    "        copy(source_label_path, target_label_folder)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocess_data(data_folder='./data')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from shutil import move\n",
    "\n",
    "from yolov5.train import run\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        data_folder='./data', batch_size=0, epochs=0, base_model='yolov5m'):\n",
    "    print('training model')\n",
    "\n",
    "    batch_size = batch_size or int(environ.get('batch_size', 4))\n",
    "    epochs = epochs or int(environ.get('epochs', 2))\n",
    "    base_model = base_model or environ.get('base_model', 'yolov5m')\n",
    "\n",
    "    run(\n",
    "        data='configuration.yaml',\n",
    "        weights=f'{base_model}.pt',\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        freeze=[10],\n",
    "        cache='disk',\n",
    "    )\n",
    "\n",
    "    move('yolov5/runs/train/exp/weights/best.pt', 'model.pt')\n",
    "\n",
    "    print('model training done')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_model(data_folder='./data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_model(model_file_path='model.pt'):\n",
    "    print('converting model')\n",
    "\n",
    "    run(\n",
    "        weights=model_file_path,\n",
    "        include=['onnx'],\n",
    "        imgsz=(640, 640),\n",
    "        opset=13,\n",
    "    )\n",
    "\n",
    "    print('model converted')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "convert_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
